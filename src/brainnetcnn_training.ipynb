{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HBl42d0-apO",
        "outputId": "731a2d1b-d645-428d-fa40-e5bfda3041a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.backends.cudnn as cudnn\n",
        "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
        "                             accuracy_score, balanced_accuracy_score, roc_auc_score)\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "GWaw037IYRig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yblQ2qJX9feA",
        "outputId": "6219a1c2-4316-49fb-e054-5e5488734005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded FCgsr data with shape: (653, 109, 109), (653,)\n",
            "Amount of 1s in y:  373\n",
            "Amount of 1s in y (train):  252\n",
            "train dataset shape: torch.Size([437, 1, 109, 109]), torch.Size([437, 1])\n",
            "Loaded FCgsr data with shape: (653, 109, 109), (653,)\n",
            "Amount of 1s in y:  373\n",
            "Amount of 1s in y (validation):  121\n",
            "validation dataset shape: torch.Size([216, 1, 109, 109]), torch.Size([216, 1])\n",
            "Training on matrix type:  FCgsr\n",
            "Epoch 10/200\n",
            "Train Loss: 0.6375 | Test Loss: 0.5914\n",
            "Accuracy: 52.31% | Balanced Acc: 52.46%\n",
            "Precision: 0.5849 | Recall: 0.5124 | F1: 0.5463 | ROC-AUC: 0.5535\n",
            "Epoch 20/200\n",
            "Train Loss: 0.5724 | Test Loss: 0.6024\n",
            "Accuracy: 51.85% | Balanced Acc: 52.84%\n",
            "Precision: 0.5934 | Recall: 0.4463 | F1: 0.5094 | ROC-AUC: 0.5297\n",
            "Epoch 30/200\n",
            "Train Loss: 0.5322 | Test Loss: 1.8196\n",
            "Accuracy: 48.61% | Balanced Acc: 52.55%\n",
            "Precision: 0.6316 | Recall: 0.1983 | F1: 0.3019 | ROC-AUC: 0.4829\n",
            "Epoch 40/200\n",
            "Train Loss: 0.0768 | Test Loss: 4.3853\n",
            "Accuracy: 50.93% | Balanced Acc: 52.92%\n",
            "Precision: 0.6027 | Recall: 0.3636 | F1: 0.4536 | ROC-AUC: 0.5052\n",
            "Epoch 50/200\n",
            "Train Loss: 0.0016 | Test Loss: 6.0964\n",
            "Accuracy: 50.00% | Balanced Acc: 48.81%\n",
            "Precision: 0.5504 | Recall: 0.5868 | F1: 0.5680 | ROC-AUC: 0.4986\n",
            "Epoch 60/200\n",
            "Train Loss: 0.0001 | Test Loss: 6.9826\n",
            "Accuracy: 50.00% | Balanced Acc: 48.02%\n",
            "Precision: 0.5455 | Recall: 0.6446 | F1: 0.5909 | ROC-AUC: 0.4962\n",
            "Epoch 70/200\n",
            "Train Loss: 0.0003 | Test Loss: 7.8933\n",
            "Accuracy: 50.00% | Balanced Acc: 48.25%\n",
            "Precision: 0.5468 | Recall: 0.6281 | F1: 0.5846 | ROC-AUC: 0.5030\n",
            "Epoch 80/200\n",
            "Train Loss: 0.0000 | Test Loss: 8.4995\n",
            "Accuracy: 51.39% | Balanced Acc: 49.15%\n",
            "Precision: 0.5541 | Recall: 0.6777 | F1: 0.6097 | ROC-AUC: 0.5003\n",
            "Epoch 90/200\n",
            "Train Loss: 0.0000 | Test Loss: 9.0508\n",
            "Accuracy: 50.93% | Balanced Acc: 48.28%\n",
            "Precision: 0.5484 | Recall: 0.7025 | F1: 0.6159 | ROC-AUC: 0.5087\n",
            "Epoch 100/200\n",
            "Train Loss: 0.0000 | Test Loss: 8.8143\n",
            "Accuracy: 50.93% | Balanced Acc: 48.85%\n",
            "Precision: 0.5517 | Recall: 0.6612 | F1: 0.6015 | ROC-AUC: 0.5071\n",
            "Epoch 110/200\n",
            "Train Loss: 0.0000 | Test Loss: 9.5397\n",
            "Accuracy: 50.93% | Balanced Acc: 48.85%\n",
            "Precision: 0.5517 | Recall: 0.6612 | F1: 0.6015 | ROC-AUC: 0.4907\n",
            "Epoch 120/200\n",
            "Train Loss: 0.0000 | Test Loss: 9.2698\n",
            "Accuracy: 50.00% | Balanced Acc: 48.02%\n",
            "Precision: 0.5455 | Recall: 0.6446 | F1: 0.5909 | ROC-AUC: 0.5073\n",
            "Epoch 130/200\n",
            "Train Loss: 0.0000 | Test Loss: 9.3193\n",
            "Accuracy: 50.46% | Balanced Acc: 48.43%\n",
            "Precision: 0.5486 | Recall: 0.6529 | F1: 0.5962 | ROC-AUC: 0.5215\n",
            "Epoch 140/200\n",
            "Train Loss: 0.0000 | Test Loss: 9.0372\n",
            "Accuracy: 50.93% | Balanced Acc: 48.73%\n",
            "Precision: 0.5510 | Recall: 0.6694 | F1: 0.6045 | ROC-AUC: 0.4964\n",
            "Epoch 150/200\n",
            "Train Loss: 0.0000 | Test Loss: 9.7158\n",
            "Accuracy: 50.46% | Balanced Acc: 48.55%\n",
            "Precision: 0.5493 | Recall: 0.6446 | F1: 0.5932 | ROC-AUC: 0.5095\n",
            "Epoch 160/200\n",
            "Train Loss: 0.0000 | Test Loss: 9.8590\n",
            "Accuracy: 49.07% | Balanced Acc: 47.65%\n",
            "Precision: 0.5414 | Recall: 0.5950 | F1: 0.5669 | ROC-AUC: 0.5049\n",
            "Epoch 170/200\n",
            "Train Loss: 0.0000 | Test Loss: 9.7620\n",
            "Accuracy: 50.00% | Balanced Acc: 47.34%\n",
            "Precision: 0.5419 | Recall: 0.6942 | F1: 0.6087 | ROC-AUC: 0.5130\n",
            "Epoch 180/200\n",
            "Train Loss: 0.0000 | Test Loss: 11.1239\n",
            "Accuracy: 51.85% | Balanced Acc: 49.79%\n",
            "Precision: 0.5586 | Recall: 0.6694 | F1: 0.6090 | ROC-AUC: 0.5013\n",
            "Epoch 190/200\n",
            "Train Loss: 0.0000 | Test Loss: 10.5541\n",
            "Accuracy: 51.85% | Balanced Acc: 49.79%\n",
            "Precision: 0.5586 | Recall: 0.6694 | F1: 0.6090 | ROC-AUC: 0.4886\n",
            "Epoch 200/200\n",
            "Train Loss: 0.0000 | Test Loss: 10.9307\n",
            "Accuracy: 50.93% | Balanced Acc: 48.73%\n",
            "Precision: 0.5510 | Recall: 0.6694 | F1: 0.6045 | ROC-AUC: 0.4806\n"
          ]
        }
      ],
      "source": [
        "dir = \"/content/drive/MyDrive/aligned\"\n",
        "matrix_type = \"FCgsr\"\n",
        "\n",
        "# Determine device for PyTorch (CUDA GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class E2EBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, example, bias=False):\n",
        "        super().__init__()\n",
        "        # Use the 4th dimension of the example input to determine the kernel width/height.\n",
        "        self.d = example.size(3)\n",
        "        self.cnn1 = nn.Conv2d(in_planes, planes, (1, self.d), bias=bias)\n",
        "        self.cnn2 = nn.Conv2d(in_planes, planes, (self.d, 1), bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = self.cnn1(x)\n",
        "        b = self.cnn2(x)\n",
        "        # Explicit dimension specification for concatenation\n",
        "        return torch.cat([a]*self.d, dim=3) + torch.cat([b]*self.d, dim=2)\n",
        "\n",
        "class BrainNetCNN(nn.Module):\n",
        "    def __init__(self, example):\n",
        "        super().__init__()\n",
        "        # Use the example input to determine the number of input channels and spatial dimensions.\n",
        "        self.in_planes = example.size(1)\n",
        "        self.d = example.size(3)\n",
        "\n",
        "        self.E2Econv1 = E2EBlock(1, 32, example, bias=True)\n",
        "        self.E2Econv2 = E2EBlock(32, 64, example, bias=True)\n",
        "        self.E2N = nn.Conv2d(64, 1, (1, self.d))\n",
        "        self.N2G = nn.Conv2d(1, 256, (self.d, 1))\n",
        "        self.dense1 = nn.Linear(256, 128)\n",
        "        self.dense2 = nn.Linear(128, 30)\n",
        "        self.dense3 = nn.Linear(30, 1) # Output a single logit\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.leaky_relu(self.E2Econv1(x), negative_slope=0.33)\n",
        "        out = F.leaky_relu(self.E2Econv2(out), negative_slope=0.33)\n",
        "        out = F.leaky_relu(self.E2N(out), negative_slope=0.33)\n",
        "        out = F.dropout(F.leaky_relu(self.N2G(out), negative_slope=0.33), p=0.5)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.dropout(F.leaky_relu(self.dense1(out), negative_slope=0.33), p=0.5)\n",
        "        out = F.dropout(F.leaky_relu(self.dense2(out), negative_slope=0.33), p=0.5)\n",
        "        out = self.dense3(out)\n",
        "        return out\n",
        "\n",
        "# Dataset and DataLoader updates\n",
        "class NCANDA_Dataset(Dataset):\n",
        "    def __init__(self, directory=dir, matrix_type=matrix_type, mode=\"train\", transform=None, class_balancing=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            directory (string): Path to the dataset.\n",
        "            mode (str): \"train\" for training, \"validation\" for validation, \"train+validation\" for full training.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.directory = directory\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "\n",
        "        x = np.load(os.path.join(directory, \"X_\" + matrix_type + \"_control_moderate.npy\"))\n",
        "        y = np.load(os.path.join(directory, \"y_aligned_control_moderate.npy\"))\n",
        "        print(f\"Loaded {matrix_type} data with shape: {x.shape}, {y.shape}\")\n",
        "        print(\"Amount of 1s in y: \", np.sum(y == 1))\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            x, y, test_size=0.33, random_state=42\n",
        "        ) # test is the validation set (for conditionals below)\n",
        "\n",
        "        if mode == \"train\":\n",
        "            x, y = X_train, y_train\n",
        "            print(\"Amount of 1s in y (train): \", np.sum(y == 1))\n",
        "        elif mode == \"validation\":\n",
        "            x, y = X_test, y_test\n",
        "            print(\"Amount of 1s in y (validation): \", np.sum(y == 1))\n",
        "        elif mode == \"train+validation\":\n",
        "            pass  # Use full dataset\n",
        "        else:\n",
        "            raise ValueError(\"Invalid mode specified\")\n",
        "\n",
        "        # NORMALIZE DATA\n",
        "        if mode == \"train\":\n",
        "            self.mean, self.std = x.mean(), x.std()\n",
        "        x = (x - X_train.mean()) / X_train.std()  # Normalize using training statistics\n",
        "\n",
        "        self.X = torch.FloatTensor(np.expand_dims(x, 1).astype(np.float32))\n",
        "        self.Y = torch.FloatTensor(y).unsqueeze(1)\n",
        "\n",
        "        print(f\"{self.mode} dataset shape: {self.X.shape}, {self.Y.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y  # Return tuple instead of list\n",
        "\n",
        "# Training and evaluation updates\n",
        "def train(epoch, net, trainloader, criterion, optimizer):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(trainloader)\n",
        "\n",
        "def test(net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            all_logits.append(outputs.cpu())\n",
        "            all_targets.append(targets.cpu())\n",
        "\n",
        "    # Concatenate and compute metrics\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    all_targets = torch.cat(all_targets, dim=0)\n",
        "\n",
        "    y_prob = torch.sigmoid(all_logits).numpy()  # Convert logits to probabilities\n",
        "    y_pred = (y_prob > 0.5).astype(int)  # Convert probabilities to binary labels\n",
        "\n",
        "    y_true = all_targets.numpy()\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "    balanced_acc = balanced_accuracy_score(y_true, y_pred) * 100\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "    avg_loss = test_loss / len(testloader)\n",
        "    # print(f\"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    # print(f\"Balanced Accuracy: {balanced_acc:.2f}%\")\n",
        "    # print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "    # print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    return avg_loss, accuracy, balanced_acc, precision, recall, f1, roc_auc\n",
        "\n",
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize datasets and loaders\n",
        "    trainset = NCANDA_Dataset(mode=\"train\")\n",
        "    testset = NCANDA_Dataset(mode=\"validation\")\n",
        "    print(\"Training on matrix type: \", matrix_type)\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=20, shuffle=True,\n",
        "                           num_workers=2, pin_memory=True)\n",
        "    testloader = DataLoader(testset, batch_size=20, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Model setup\n",
        "    # Initialize the network using an example input from the training set\n",
        "    net = BrainNetCNN(trainset.X[0:1])\n",
        "    net = net.to(device)\n",
        "    if device.type == \"cuda\":\n",
        "        net = torch.nn.DataParallel(net)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # Initialize weights using Kaiming initialization\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu', a=0.33)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "    net.apply(init_weights)\n",
        "\n",
        "    # Training parameters\n",
        "    # We have 280 of class 0 and 373 of class 1 (in training data)\n",
        "    pos_weight = torch.tensor([280 / 373], device=device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = torch.optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 200\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracies = []\n",
        "    balanced_accs = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    roc_aucs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(epoch, net, trainloader, criterion, optimizer)\n",
        "        test_metrics = test(net, testloader, criterion)\n",
        "        test_loss, accuracy, balanced_acc, precision, recall, f1, roc_auc = test_metrics\n",
        "\n",
        "        # Append metrics\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        accuracies.append(accuracy)\n",
        "        balanced_accs.append(balanced_acc)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        roc_aucs.append(roc_auc)\n",
        "\n",
        "        # Print every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "            print(f\"Accuracy: {accuracy:.2f}% | Balanced Acc: {balanced_acc:.2f}%\")\n",
        "            print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    # Save all metrics\n",
        "    torch.save(net.state_dict(), \"brainnetcnn_model.pth\")\n",
        "    np.savez(\"training_stats.npz\",\n",
        "             train_losses=train_losses,\n",
        "             test_losses=test_losses,\n",
        "             accuracies=accuracies,\n",
        "             balanced_accs=balanced_accs,\n",
        "             precisions=precisions,\n",
        "             recalls=recalls,\n",
        "             f1_scores=f1_scores,\n",
        "             roc_aucs=roc_aucs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/drive/MyDrive/aligned\"\n",
        "matrix_type = \"SC\"\n",
        "\n",
        "\n",
        "# Determine device for PyTorch (CUDA GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class E2EBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, example, bias=False):\n",
        "        super().__init__()\n",
        "        # Use the 4th dimension of the example input to determine the kernel width/height.\n",
        "        self.d = example.size(3)\n",
        "        self.cnn1 = nn.Conv2d(in_planes, planes, (1, self.d), bias=bias)\n",
        "        self.cnn2 = nn.Conv2d(in_planes, planes, (self.d, 1), bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = self.cnn1(x)\n",
        "        b = self.cnn2(x)\n",
        "        # Explicit dimension specification for concatenation\n",
        "        return torch.cat([a]*self.d, dim=3) + torch.cat([b]*self.d, dim=2)\n",
        "\n",
        "class BrainNetCNN(nn.Module):\n",
        "    def __init__(self, example):\n",
        "        super().__init__()\n",
        "        # Use the example input to determine the number of input channels and spatial dimensions.\n",
        "        self.in_planes = example.size(1)\n",
        "        self.d = example.size(3)\n",
        "\n",
        "        self.E2Econv1 = E2EBlock(1, 32, example, bias=True)\n",
        "        self.E2Econv2 = E2EBlock(32, 64, example, bias=True)\n",
        "        self.E2N = nn.Conv2d(64, 1, (1, self.d))\n",
        "        self.N2G = nn.Conv2d(1, 256, (self.d, 1))\n",
        "        self.dense1 = nn.Linear(256, 128)\n",
        "        self.dense2 = nn.Linear(128, 30)\n",
        "        self.dense3 = nn.Linear(30, 1) # Output a single logit\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.leaky_relu(self.E2Econv1(x), negative_slope=0.33)\n",
        "        out = F.leaky_relu(self.E2Econv2(out), negative_slope=0.33)\n",
        "        out = F.leaky_relu(self.E2N(out), negative_slope=0.33)\n",
        "        out = F.dropout(F.leaky_relu(self.N2G(out), negative_slope=0.33), p=0.5)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.dropout(F.leaky_relu(self.dense1(out), negative_slope=0.33), p=0.5)\n",
        "        out = F.dropout(F.leaky_relu(self.dense2(out), negative_slope=0.33), p=0.5)\n",
        "        out = self.dense3(out)\n",
        "        return out\n",
        "\n",
        "# Dataset and DataLoader updates\n",
        "class NCANDA_Dataset(Dataset):\n",
        "    def __init__(self, directory=dir, matrix_type=matrix_type, mode=\"train\", transform=None, class_balancing=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            directory (string): Path to the dataset.\n",
        "            mode (str): \"train\" for training, \"validation\" for validation, \"train+validation\" for full training.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.directory = directory\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "\n",
        "        x = np.load(os.path.join(directory, \"X_\" + matrix_type + \"_control_moderate.npy\"))\n",
        "        y = np.load(os.path.join(directory, \"y_aligned_control_moderate.npy\"))\n",
        "        print(f\"Loaded {matrix_type} data with shape: {x.shape}, {y.shape}\")\n",
        "        print(\"Amount of 1s in y: \", np.sum(y == 1))\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            x, y, test_size=0.33, random_state=42\n",
        "        ) # test is the validation set (for conditionals below)\n",
        "\n",
        "        if mode == \"train\":\n",
        "            x, y = X_train, y_train\n",
        "            print(\"Amount of 1s in y (train): \", np.sum(y == 1))\n",
        "        elif mode == \"validation\":\n",
        "            x, y = X_test, y_test\n",
        "            print(\"Amount of 1s in y (validation): \", np.sum(y == 1))\n",
        "        elif mode == \"train+validation\":\n",
        "            pass  # Use full dataset\n",
        "        else:\n",
        "            raise ValueError(\"Invalid mode specified\")\n",
        "\n",
        "        # NORMALIZE DATA\n",
        "        if mode == \"train\":\n",
        "            self.mean, self.std = x.mean(), x.std()\n",
        "        x = (x - X_train.mean()) / X_train.std()  # Normalize using training statistics\n",
        "\n",
        "        self.X = torch.FloatTensor(np.expand_dims(x, 1).astype(np.float32))\n",
        "        self.Y = torch.FloatTensor(y).unsqueeze(1)\n",
        "\n",
        "        print(f\"{self.mode} dataset shape: {self.X.shape}, {self.Y.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y  # Return tuple instead of list\n",
        "\n",
        "# Training and evaluation updates\n",
        "def train(epoch, net, trainloader, criterion, optimizer):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(trainloader)\n",
        "\n",
        "def test(net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            all_logits.append(outputs.cpu())\n",
        "            all_targets.append(targets.cpu())\n",
        "\n",
        "    # Concatenate and compute metrics\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    all_targets = torch.cat(all_targets, dim=0)\n",
        "\n",
        "    y_prob = torch.sigmoid(all_logits).numpy()  # Convert logits to probabilities\n",
        "    y_pred = (y_prob > 0.5).astype(int)  # Convert probabilities to binary labels\n",
        "\n",
        "    y_true = all_targets.numpy()\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "    balanced_acc = balanced_accuracy_score(y_true, y_pred) * 100\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "    avg_loss = test_loss / len(testloader)\n",
        "    # print(f\"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    # print(f\"Balanced Accuracy: {balanced_acc:.2f}%\")\n",
        "    # print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "    # print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    return avg_loss, accuracy, balanced_acc, precision, recall, f1, roc_auc\n",
        "\n",
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize datasets and loaders\n",
        "    trainset = NCANDA_Dataset(mode=\"train\")\n",
        "    testset = NCANDA_Dataset(mode=\"validation\")\n",
        "    print(\"Training on matrix type: \", matrix_type)\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=20, shuffle=True,\n",
        "                           num_workers=2, pin_memory=True)\n",
        "    testloader = DataLoader(testset, batch_size=20, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Model setup\n",
        "    # Initialize the network using an example input from the training set\n",
        "    net = BrainNetCNN(trainset.X[0:1])\n",
        "    net = net.to(device)\n",
        "    if device.type == \"cuda\":\n",
        "        net = torch.nn.DataParallel(net)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # Initialize weights using Kaiming initialization\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu', a=0.33)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "    net.apply(init_weights)\n",
        "\n",
        "    # Training parameters\n",
        "    # We have 280 of class 0 and 373 of class 1 (in training data)\n",
        "    pos_weight = torch.tensor([280 / 373], device=device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = torch.optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 200\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracies = []\n",
        "    balanced_accs = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    roc_aucs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(epoch, net, trainloader, criterion, optimizer)\n",
        "        test_metrics = test(net, testloader, criterion)\n",
        "        test_loss, accuracy, balanced_acc, precision, recall, f1, roc_auc = test_metrics\n",
        "\n",
        "        # Append metrics\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        accuracies.append(accuracy)\n",
        "        balanced_accs.append(balanced_acc)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        roc_aucs.append(roc_auc)\n",
        "\n",
        "        # Print every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "            print(f\"Accuracy: {accuracy:.2f}% | Balanced Acc: {balanced_acc:.2f}%\")\n",
        "            print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    # Save all metrics\n",
        "    torch.save(net.state_dict(), \"SC_brainnetcnn_model.pth\")\n",
        "    np.savez(\"SC_training_stats.npz\",\n",
        "             train_losses=train_losses,\n",
        "             test_losses=test_losses,\n",
        "             accuracies=accuracies,\n",
        "             balanced_accs=balanced_accs,\n",
        "             precisions=precisions,\n",
        "             recalls=recalls,\n",
        "             f1_scores=f1_scores,\n",
        "             roc_aucs=roc_aucs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWgFrFoOXNhR",
        "outputId": "c9f8d145-8733-4d8d-f0fe-c4b39bac6871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded SC data with shape: (653, 90, 90), (653,)\n",
            "Amount of 1s in y:  373\n",
            "Amount of 1s in y (train):  252\n",
            "train dataset shape: torch.Size([437, 1, 90, 90]), torch.Size([437, 1])\n",
            "Loaded SC data with shape: (653, 90, 90), (653,)\n",
            "Amount of 1s in y:  373\n",
            "Amount of 1s in y (validation):  121\n",
            "validation dataset shape: torch.Size([216, 1, 90, 90]), torch.Size([216, 1])\n",
            "Training on matrix type:  SC\n",
            "Epoch 10/200\n",
            "Train Loss: 0.6283 | Test Loss: 0.5974\n",
            "Accuracy: 47.69% | Balanced Acc: 47.65%\n",
            "Precision: 0.5370 | Recall: 0.4793 | F1: 0.5066 | ROC-AUC: 0.5115\n",
            "Epoch 20/200\n",
            "Train Loss: 0.5982 | Test Loss: 0.6172\n",
            "Accuracy: 46.76% | Balanced Acc: 47.16%\n",
            "Precision: 0.5300 | Recall: 0.4380 | F1: 0.4796 | ROC-AUC: 0.4483\n",
            "Epoch 30/200\n",
            "Train Loss: 0.6055 | Test Loss: 0.6021\n",
            "Accuracy: 51.85% | Balanced Acc: 50.69%\n",
            "Precision: 0.5659 | Recall: 0.6033 | F1: 0.5840 | ROC-AUC: 0.5134\n",
            "Epoch 40/200\n",
            "Train Loss: 0.6130 | Test Loss: 0.5973\n",
            "Accuracy: 52.78% | Balanced Acc: 51.52%\n",
            "Precision: 0.5725 | Recall: 0.6198 | F1: 0.5952 | ROC-AUC: 0.5414\n",
            "Epoch 50/200\n",
            "Train Loss: 0.6041 | Test Loss: 0.6112\n",
            "Accuracy: 47.22% | Balanced Acc: 48.03%\n",
            "Precision: 0.5376 | Recall: 0.4132 | F1: 0.4673 | ROC-AUC: 0.4594\n",
            "Epoch 60/200\n",
            "Train Loss: 0.5953 | Test Loss: 0.5947\n",
            "Accuracy: 56.48% | Balanced Acc: 55.39%\n",
            "Precision: 0.6047 | Recall: 0.6446 | F1: 0.6240 | ROC-AUC: 0.5419\n",
            "Epoch 70/200\n",
            "Train Loss: 0.5924 | Test Loss: 0.6064\n",
            "Accuracy: 50.00% | Balanced Acc: 46.89%\n",
            "Precision: 0.5399 | Recall: 0.7273 | F1: 0.6197 | ROC-AUC: 0.4286\n",
            "Epoch 80/200\n",
            "Train Loss: 0.5949 | Test Loss: 0.5932\n",
            "Accuracy: 52.31% | Balanced Acc: 51.90%\n",
            "Precision: 0.5776 | Recall: 0.5537 | F1: 0.5654 | ROC-AUC: 0.5420\n",
            "Epoch 90/200\n",
            "Train Loss: 0.5954 | Test Loss: 0.5959\n",
            "Accuracy: 47.69% | Balanced Acc: 48.22%\n",
            "Precision: 0.5408 | Recall: 0.4380 | F1: 0.4840 | ROC-AUC: 0.5174\n",
            "Epoch 100/200\n",
            "Train Loss: 0.5982 | Test Loss: 0.6012\n",
            "Accuracy: 51.39% | Balanced Acc: 48.47%\n",
            "Precision: 0.5500 | Recall: 0.7273 | F1: 0.6263 | ROC-AUC: 0.4360\n",
            "Epoch 110/200\n",
            "Train Loss: 0.5847 | Test Loss: 0.5959\n",
            "Accuracy: 50.46% | Balanced Acc: 53.52%\n",
            "Precision: 0.6296 | Recall: 0.2810 | F1: 0.3886 | ROC-AUC: 0.5636\n",
            "Epoch 120/200\n",
            "Train Loss: 0.5948 | Test Loss: 0.5932\n",
            "Accuracy: 48.15% | Balanced Acc: 50.55%\n",
            "Precision: 0.5692 | Recall: 0.3058 | F1: 0.3978 | ROC-AUC: 0.5626\n",
            "Epoch 130/200\n",
            "Train Loss: 0.6198 | Test Loss: 0.5958\n",
            "Accuracy: 51.39% | Balanced Acc: 53.90%\n",
            "Precision: 0.6250 | Recall: 0.3306 | F1: 0.4324 | ROC-AUC: 0.5204\n",
            "Epoch 140/200\n",
            "Train Loss: 0.5546 | Test Loss: 0.5864\n",
            "Accuracy: 62.04% | Balanced Acc: 57.86%\n",
            "Precision: 0.6054 | Recall: 0.9256 | F1: 0.7320 | ROC-AUC: 0.6245\n",
            "Epoch 150/200\n",
            "Train Loss: 0.5496 | Test Loss: 0.5921\n",
            "Accuracy: 53.24% | Balanced Acc: 54.08%\n",
            "Precision: 0.6064 | Recall: 0.4711 | F1: 0.5302 | ROC-AUC: 0.5851\n",
            "Epoch 160/200\n",
            "Train Loss: 0.4581 | Test Loss: 0.6834\n",
            "Accuracy: 55.09% | Balanced Acc: 54.15%\n",
            "Precision: 0.5952 | Recall: 0.6198 | F1: 0.6073 | ROC-AUC: 0.5649\n",
            "Epoch 170/200\n",
            "Train Loss: 0.3703 | Test Loss: 0.8048\n",
            "Accuracy: 59.72% | Balanced Acc: 57.72%\n",
            "Precision: 0.6164 | Recall: 0.7438 | F1: 0.6742 | ROC-AUC: 0.5832\n",
            "Epoch 180/200\n",
            "Train Loss: 0.2204 | Test Loss: 1.8916\n",
            "Accuracy: 57.87% | Balanced Acc: 54.03%\n",
            "Precision: 0.5843 | Recall: 0.8595 | F1: 0.6957 | ROC-AUC: 0.5600\n",
            "Epoch 190/200\n",
            "Train Loss: 0.1317 | Test Loss: 2.0460\n",
            "Accuracy: 56.94% | Balanced Acc: 53.99%\n",
            "Precision: 0.5864 | Recall: 0.7851 | F1: 0.6714 | ROC-AUC: 0.5556\n",
            "Epoch 200/200\n",
            "Train Loss: 0.0002 | Test Loss: 5.8474\n",
            "Accuracy: 53.70% | Balanced Acc: 52.57%\n",
            "Precision: 0.5814 | Recall: 0.6198 | F1: 0.6000 | ROC-AUC: 0.5410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/drive/MyDrive/aligned\"\n",
        "matrix_type = \"FC\"\n",
        "\n",
        "\n",
        "# Determine device for PyTorch (CUDA GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class E2EBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, example, bias=False):\n",
        "        super().__init__()\n",
        "        # Use the 4th dimension of the example input to determine the kernel width/height.\n",
        "        self.d = example.size(3)\n",
        "        self.cnn1 = nn.Conv2d(in_planes, planes, (1, self.d), bias=bias)\n",
        "        self.cnn2 = nn.Conv2d(in_planes, planes, (self.d, 1), bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = self.cnn1(x)\n",
        "        b = self.cnn2(x)\n",
        "        # Explicit dimension specification for concatenation\n",
        "        return torch.cat([a]*self.d, dim=3) + torch.cat([b]*self.d, dim=2)\n",
        "\n",
        "class BrainNetCNN(nn.Module):\n",
        "    def __init__(self, example):\n",
        "        super().__init__()\n",
        "        # Use the example input to determine the number of input channels and spatial dimensions.\n",
        "        self.in_planes = example.size(1)\n",
        "        self.d = example.size(3)\n",
        "\n",
        "        self.E2Econv1 = E2EBlock(1, 32, example, bias=True)\n",
        "        self.E2Econv2 = E2EBlock(32, 64, example, bias=True)\n",
        "        self.E2N = nn.Conv2d(64, 1, (1, self.d))\n",
        "        self.N2G = nn.Conv2d(1, 256, (self.d, 1))\n",
        "        self.dense1 = nn.Linear(256, 128)\n",
        "        self.dense2 = nn.Linear(128, 30)\n",
        "        self.dense3 = nn.Linear(30, 1) # Output a single logit\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.leaky_relu(self.E2Econv1(x), negative_slope=0.33)\n",
        "        out = F.leaky_relu(self.E2Econv2(out), negative_slope=0.33)\n",
        "        out = F.leaky_relu(self.E2N(out), negative_slope=0.33)\n",
        "        out = F.dropout(F.leaky_relu(self.N2G(out), negative_slope=0.33), p=0.5)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.dropout(F.leaky_relu(self.dense1(out), negative_slope=0.33), p=0.5)\n",
        "        out = F.dropout(F.leaky_relu(self.dense2(out), negative_slope=0.33), p=0.5)\n",
        "        out = self.dense3(out)\n",
        "        return out\n",
        "\n",
        "# Dataset and DataLoader updates\n",
        "class NCANDA_Dataset(Dataset):\n",
        "    def __init__(self, directory=dir, matrix_type=matrix_type, mode=\"train\", transform=None, class_balancing=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            directory (string): Path to the dataset.\n",
        "            mode (str): \"train\" for training, \"validation\" for validation, \"train+validation\" for full training.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.directory = directory\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "\n",
        "        x = np.load(os.path.join(directory, \"X_\" + matrix_type + \"_control_moderate.npy\"))\n",
        "        y = np.load(os.path.join(directory, \"y_aligned_control_moderate.npy\"))\n",
        "        print(f\"Loaded {matrix_type} data with shape: {x.shape}, {y.shape}\")\n",
        "        print(\"Amount of 1s in y: \", np.sum(y == 1))\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            x, y, test_size=0.33, random_state=42\n",
        "        ) # test is the validation set (for conditionals below)\n",
        "\n",
        "        if mode == \"train\":\n",
        "            x, y = X_train, y_train\n",
        "            print(\"Amount of 1s in y (train): \", np.sum(y == 1))\n",
        "        elif mode == \"validation\":\n",
        "            x, y = X_test, y_test\n",
        "            print(\"Amount of 1s in y (validation): \", np.sum(y == 1))\n",
        "        elif mode == \"train+validation\":\n",
        "            pass  # Use full dataset\n",
        "        else:\n",
        "            raise ValueError(\"Invalid mode specified\")\n",
        "\n",
        "        # NORMALIZE DATA\n",
        "        if mode == \"train\":\n",
        "            self.mean, self.std = x.mean(), x.std()\n",
        "        x = (x - X_train.mean()) / X_train.std()  # Normalize using training statistics\n",
        "\n",
        "        self.X = torch.FloatTensor(np.expand_dims(x, 1).astype(np.float32))\n",
        "        self.Y = torch.FloatTensor(y).unsqueeze(1)\n",
        "\n",
        "        print(f\"{self.mode} dataset shape: {self.X.shape}, {self.Y.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y  # Return tuple instead of list\n",
        "\n",
        "# Training and evaluation updates\n",
        "def train(epoch, net, trainloader, criterion, optimizer):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(trainloader)\n",
        "\n",
        "def test(net, testloader, criterion):\n",
        "    net.eval()\n",
        "    test_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            all_logits.append(outputs.cpu())\n",
        "            all_targets.append(targets.cpu())\n",
        "\n",
        "    # Concatenate and compute metrics\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    all_targets = torch.cat(all_targets, dim=0)\n",
        "\n",
        "    y_prob = torch.sigmoid(all_logits).numpy()  # Convert logits to probabilities\n",
        "    y_pred = (y_prob > 0.5).astype(int)  # Convert probabilities to binary labels\n",
        "\n",
        "    y_true = all_targets.numpy()\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "    balanced_acc = balanced_accuracy_score(y_true, y_pred) * 100\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "    avg_loss = test_loss / len(testloader)\n",
        "    # print(f\"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "    # print(f\"Balanced Accuracy: {balanced_acc:.2f}%\")\n",
        "    # print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "    # print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    return avg_loss, accuracy, balanced_acc, precision, recall, f1, roc_auc\n",
        "\n",
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize datasets and loaders\n",
        "    trainset = NCANDA_Dataset(mode=\"train\")\n",
        "    testset = NCANDA_Dataset(mode=\"validation\")\n",
        "    print(\"Training on matrix type: \", matrix_type)\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=20, shuffle=True,\n",
        "                           num_workers=2, pin_memory=True)\n",
        "    testloader = DataLoader(testset, batch_size=20, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Model setup\n",
        "    # Initialize the network using an example input from the training set\n",
        "    net = BrainNetCNN(trainset.X[0:1])\n",
        "    net = net.to(device)\n",
        "    if device.type == \"cuda\":\n",
        "        net = torch.nn.DataParallel(net)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # Initialize weights using Kaiming initialization\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu', a=0.33)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "    net.apply(init_weights)\n",
        "\n",
        "    # Training parameters\n",
        "    # We have 280 of class 0 and 373 of class 1 (in training data)\n",
        "    pos_weight = torch.tensor([280 / 373], device=device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = torch.optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 200\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracies = []\n",
        "    balanced_accs = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    roc_aucs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(epoch, net, trainloader, criterion, optimizer)\n",
        "        test_metrics = test(net, testloader, criterion)\n",
        "        test_loss, accuracy, balanced_acc, precision, recall, f1, roc_auc = test_metrics\n",
        "\n",
        "        # Append metrics\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        accuracies.append(accuracy)\n",
        "        balanced_accs.append(balanced_acc)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        roc_aucs.append(roc_auc)\n",
        "\n",
        "        # Print every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "            print(f\"Accuracy: {accuracy:.2f}% | Balanced Acc: {balanced_acc:.2f}%\")\n",
        "            print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    # Save all metrics\n",
        "    torch.save(net.state_dict(), \"FC_brainnetcnn_model.pth\")\n",
        "    np.savez(\"FC_training_stats.npz\",\n",
        "             train_losses=train_losses,\n",
        "             test_losses=test_losses,\n",
        "             accuracies=accuracies,\n",
        "             balanced_accs=balanced_accs,\n",
        "             precisions=precisions,\n",
        "             recalls=recalls,\n",
        "             f1_scores=f1_scores,\n",
        "             roc_aucs=roc_aucs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iKADyMWXUw8",
        "outputId": "4ed316b6-b9d4-48e9-c4dd-a8b1557f7871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded FC data with shape: (653, 109, 109), (653,)\n",
            "Amount of 1s in y:  373\n",
            "Amount of 1s in y (train):  252\n",
            "train dataset shape: torch.Size([437, 1, 109, 109]), torch.Size([437, 1])\n",
            "Loaded FC data with shape: (653, 109, 109), (653,)\n",
            "Amount of 1s in y:  373\n",
            "Amount of 1s in y (validation):  121\n",
            "validation dataset shape: torch.Size([216, 1, 109, 109]), torch.Size([216, 1])\n",
            "Training on matrix type:  FC\n",
            "Epoch 10/200\n",
            "Train Loss: 0.6415 | Test Loss: 0.5956\n",
            "Accuracy: 49.54% | Balanced Acc: 48.51%\n",
            "Precision: 0.5476 | Recall: 0.5702 | F1: 0.5587 | ROC-AUC: 0.5126\n",
            "Epoch 20/200\n",
            "Train Loss: 0.6106 | Test Loss: 0.6000\n",
            "Accuracy: 47.22% | Balanced Acc: 49.50%\n",
            "Precision: 0.5522 | Recall: 0.3058 | F1: 0.3936 | ROC-AUC: 0.5401\n",
            "Epoch 30/200\n",
            "Train Loss: 0.6149 | Test Loss: 0.6888\n",
            "Accuracy: 56.48% | Balanced Acc: 53.13%\n",
            "Precision: 0.5799 | Recall: 0.8099 | F1: 0.6759 | ROC-AUC: 0.5597\n",
            "Epoch 40/200\n",
            "Train Loss: 0.1281 | Test Loss: 2.8973\n",
            "Accuracy: 48.61% | Balanced Acc: 49.27%\n",
            "Precision: 0.5521 | Recall: 0.4380 | F1: 0.4885 | ROC-AUC: 0.4917\n",
            "Epoch 50/200\n",
            "Train Loss: 0.2038 | Test Loss: 1.7975\n",
            "Accuracy: 52.78% | Balanced Acc: 51.86%\n",
            "Precision: 0.5760 | Recall: 0.5950 | F1: 0.5854 | ROC-AUC: 0.5254\n",
            "Epoch 60/200\n",
            "Train Loss: 0.0004 | Test Loss: 4.8585\n",
            "Accuracy: 52.78% | Balanced Acc: 50.61%\n",
            "Precision: 0.5646 | Recall: 0.6860 | F1: 0.6194 | ROC-AUC: 0.5467\n",
            "Epoch 70/200\n",
            "Train Loss: 0.0013 | Test Loss: 5.6727\n",
            "Accuracy: 52.78% | Balanced Acc: 51.29%\n",
            "Precision: 0.5704 | Recall: 0.6364 | F1: 0.6016 | ROC-AUC: 0.5307\n",
            "Epoch 80/200\n",
            "Train Loss: 0.0000 | Test Loss: 6.1668\n",
            "Accuracy: 53.24% | Balanced Acc: 50.80%\n",
            "Precision: 0.5658 | Recall: 0.7107 | F1: 0.6300 | ROC-AUC: 0.5176\n",
            "Epoch 90/200\n",
            "Train Loss: 0.0000 | Test Loss: 6.4909\n",
            "Accuracy: 52.78% | Balanced Acc: 51.29%\n",
            "Precision: 0.5704 | Recall: 0.6364 | F1: 0.6016 | ROC-AUC: 0.5261\n",
            "Epoch 100/200\n",
            "Train Loss: 0.0000 | Test Loss: 6.9909\n",
            "Accuracy: 53.24% | Balanced Acc: 51.03%\n",
            "Precision: 0.5676 | Recall: 0.6942 | F1: 0.6245 | ROC-AUC: 0.5236\n",
            "Epoch 110/200\n",
            "Train Loss: 0.0001 | Test Loss: 12.1826\n",
            "Accuracy: 52.31% | Balanced Acc: 50.31%\n",
            "Precision: 0.5625 | Recall: 0.6694 | F1: 0.6113 | ROC-AUC: 0.4988\n",
            "Epoch 120/200\n",
            "Train Loss: 0.0000 | Test Loss: 11.4820\n",
            "Accuracy: 52.31% | Balanced Acc: 50.65%\n",
            "Precision: 0.5652 | Recall: 0.6446 | F1: 0.6023 | ROC-AUC: 0.5023\n",
            "Epoch 130/200\n",
            "Train Loss: 0.0001 | Test Loss: 10.9952\n",
            "Accuracy: 52.31% | Balanced Acc: 50.54%\n",
            "Precision: 0.5643 | Recall: 0.6529 | F1: 0.6054 | ROC-AUC: 0.5012\n",
            "Epoch 140/200\n",
            "Train Loss: 0.0000 | Test Loss: 11.7834\n",
            "Accuracy: 52.31% | Balanced Acc: 50.31%\n",
            "Precision: 0.5625 | Recall: 0.6694 | F1: 0.6113 | ROC-AUC: 0.5014\n",
            "Epoch 150/200\n",
            "Train Loss: 0.0410 | Test Loss: 2.9512\n",
            "Accuracy: 52.78% | Balanced Acc: 50.27%\n",
            "Precision: 0.5621 | Recall: 0.7107 | F1: 0.6277 | ROC-AUC: 0.5278\n",
            "Epoch 160/200\n",
            "Train Loss: 0.0005 | Test Loss: 4.6939\n",
            "Accuracy: 52.78% | Balanced Acc: 50.27%\n",
            "Precision: 0.5621 | Recall: 0.7107 | F1: 0.6277 | ROC-AUC: 0.5237\n",
            "Epoch 170/200\n",
            "Train Loss: 0.0006 | Test Loss: 5.2354\n",
            "Accuracy: 54.17% | Balanced Acc: 51.40%\n",
            "Precision: 0.5696 | Recall: 0.7438 | F1: 0.6452 | ROC-AUC: 0.5447\n",
            "Epoch 180/200\n",
            "Train Loss: 0.0002 | Test Loss: 5.9740\n",
            "Accuracy: 52.31% | Balanced Acc: 49.86%\n",
            "Precision: 0.5592 | Recall: 0.7025 | F1: 0.6227 | ROC-AUC: 0.5306\n",
            "Epoch 190/200\n",
            "Train Loss: 0.0001 | Test Loss: 5.9094\n",
            "Accuracy: 54.17% | Balanced Acc: 51.51%\n",
            "Precision: 0.5705 | Recall: 0.7355 | F1: 0.6426 | ROC-AUC: 0.5202\n",
            "Epoch 200/200\n",
            "Train Loss: 0.0000 | Test Loss: 6.5624\n",
            "Accuracy: 53.70% | Balanced Acc: 50.87%\n",
            "Precision: 0.5660 | Recall: 0.7438 | F1: 0.6429 | ROC-AUC: 0.5361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SC_state_dict = torch.load(\"/content/SC_brainnetcnn_model.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "print(SC_state_dict.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B9gm-mYb0i8",
        "outputId": "99336dca-4eb8-4394-9214-3edc80ebc381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['module.E2Econv1.cnn1.weight', 'module.E2Econv1.cnn1.bias', 'module.E2Econv1.cnn2.weight', 'module.E2Econv1.cnn2.bias', 'module.E2Econv2.cnn1.weight', 'module.E2Econv2.cnn1.bias', 'module.E2Econv2.cnn2.weight', 'module.E2Econv2.cnn2.bias', 'module.E2N.weight', 'module.E2N.bias', 'module.N2G.weight', 'module.N2G.bias', 'module.dense1.weight', 'module.dense1.bias', 'module.dense2.weight', 'module.dense2.bias', 'module.dense3.weight', 'module.dense3.bias'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-911499977908>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  SC_state_dict = torch.load(\"/content/SC_brainnetcnn_model.pth\", map_location=torch.device('cpu'))\n"
          ]
        }
      ]
    }
  ]
}